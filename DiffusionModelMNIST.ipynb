{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "847d8635",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader \n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm \n",
    "import torch.nn as nn \n",
    "import os \n",
    "import time \n",
    "import math\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import imageio\n",
    "import shutil\n",
    "\n",
    "if os.path.exists('results'):\n",
    "    shutil.rmtree('results')\n",
    "else:\n",
    "    os.mkdir('results')\n",
    "\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "NUM_STEPS = 100\n",
    "EPOCHS = 100\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d01e7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "MNIST_DATA = MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "MNIST_LOADER = DataLoader(MNIST_DATA, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773677c0",
   "metadata": {},
   "source": [
    "# Making a Unet Network for 28x28 image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95cb8e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv(nn.Module):\n",
    "    def __init__(self, inc,ouc):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=inc,out_channels=ouc,kernel_size=3,padding=1),\n",
    "            nn.GroupNorm(1,ouc),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(in_channels=ouc,out_channels=ouc,kernel_size=3,padding=1),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.conv1(x)\n",
    "    \n",
    "class Down(nn.Module):\n",
    "    def __init__(self, inc, ouc,emb_dim=512):\n",
    "        super().__init__()\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.conv = Conv(inc,ouc)\n",
    "        self.embedding_layer = nn.Sequential(\n",
    "            nn.SELU(),\n",
    "            nn.Linear(emb_dim,ouc)\n",
    "            )\n",
    "    def forward(self,x,time_step):\n",
    "        x = self.pool(x)\n",
    "        x = self.conv(x)\n",
    "        embedding = self.embedding_layer(time_step).squeeze(1)[:,:,None,None].repeat(1,1,x.shape[-2],x.shape[-1])\n",
    "        return x+embedding\n",
    "    \n",
    "class Up(nn.Module):\n",
    "    def __init__(self, inc,ouc,emb_dim=512):\n",
    "        super().__init__()\n",
    "        self.up = nn.Upsample(scale_factor=2,mode='bilinear')\n",
    "        self.conv = nn.Sequential(\n",
    "            Conv(inc,ouc)\n",
    "        )\n",
    "        self.embedding_layer = nn.Sequential(\n",
    "            nn.SELU(),\n",
    "            nn.Linear(emb_dim,ouc)\n",
    "            )\n",
    "    def forward(self,x,skip,t):\n",
    "        x = self.up(x)\n",
    "        x = torch.cat([skip,x],dim=1)\n",
    "        x = self.conv(x)\n",
    "        emb = self.embedding_layer(t).squeeze(1)[:,:,None,None].repeat(1,1,x.shape[-2],x.shape[-1])\n",
    "        return x+emb\n",
    "        \n",
    "\n",
    "class Unet(nn.Module):\n",
    "    def __init__(self, inc=1,ouc=1,emb_dim=512,device='cpu',steps=NUM_STEPS):\n",
    "        super().__init__()\n",
    "        self.time_emb_layer = nn.Embedding(steps,embedding_dim=emb_dim)\n",
    "\n",
    "        self.c1 = Conv(inc,64)\n",
    "        self.down1 = Down(64,128)\n",
    "        self.c2 = Conv(128,256)\n",
    "        self.down2 = Down(256,256)\n",
    "\n",
    "        self.b1 = Conv(256,512)\n",
    "        self.b2 = Conv(512,512)\n",
    "        self.b3 = Conv(512,256)\n",
    "\n",
    "        self.up1 = Up(512,128)\n",
    "        self.c3 = Conv(128,64)\n",
    "        self.up2 = Up(128,64)\n",
    "\n",
    "        self.ouc = nn.Conv2d(64,ouc,kernel_size=1)\n",
    "\n",
    "    def forward(self,x,t):\n",
    "        t = t.unsqueeze(-1)\n",
    "        t = self.time_emb_layer(t)\n",
    "\n",
    "        x1 = self.c1(x)\n",
    "        x2 = self.down1(x1,t)\n",
    "        x2 = self.c2(x2)\n",
    "        x3 = self.down2(x2,t)\n",
    "\n",
    "        x4 = self.b1(x3)\n",
    "        x4 = self.b2(x4)\n",
    "        x4 = self.b3(x4)    \n",
    "\n",
    "        x = self.up1(x4,x2,t)\n",
    "        x = self.c3(x)\n",
    "        x = self.up2(x,x1,t)\n",
    "        x = self.ouc(x)\n",
    "        return x\n",
    "\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb9238cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionModel:\n",
    "    def __init__(self,num_steps=NUM_STEPS,device=DEVICE,beta_start=1e-4,beta_end=0.02,image_size=28):\n",
    "        self.num_steps = num_steps\n",
    "        self.device = device\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_end = beta_end\n",
    "        self.image_size = image_size\n",
    "        self.beta  = self.noise_scheduler().to(device)\n",
    "        self.alpha = 1. - self.beta \n",
    "        self.alpha_hat = torch.cumprod(self.alpha , dim=0)\n",
    "        self.lr = 3e-4\n",
    "\n",
    "    def noise_scheduler(self, s: float = 0.008):\n",
    "        steps = self.num_steps + 1\n",
    "        x = torch.linspace(0, self.num_steps, steps, device=self.device)\n",
    "        alphas_cumprod = torch.cos(\n",
    "            ((x / self.num_steps + s) / (1 + s)) * math.pi * 0.5\n",
    "        ) ** 2\n",
    "        alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "        betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "        betas = torch.clip(betas, 1e-8, 0.999)  \n",
    "        return betas\n",
    "\n",
    "    def noise_images(self,x,t):\n",
    "        sqrt_alpha_hat = torch.sqrt(self.alpha_hat[t])[:,None,None,None]\n",
    "        sqrt_one_minus_alpha_hat = torch.sqrt(1-self.alpha_hat[t])[:,None,None,None]\n",
    "        E = torch.randn_like(x)\n",
    "        return sqrt_alpha_hat *x + sqrt_one_minus_alpha_hat* E , E\n",
    "\n",
    "    def sample_timestemps(self,n):\n",
    "        return torch.randint(low=1,high=self.num_steps,size=(n,))\n",
    "    \n",
    "    def save_images(self,images, path, **kwargs):\n",
    "        grid = torchvision.utils.make_grid(images, **kwargs)\n",
    "        ndarr = grid.permute(1, 2, 0).to('cpu').numpy()\n",
    "        im = Image.fromarray(ndarr)\n",
    "        im.save(path)\n",
    "\n",
    "    def train(self,unet_model,dataloader,epochs):\n",
    "        unet_model = unet_model.to(self.device)\n",
    "        optimizer = torch.optim.AdamW(unet_model.parameters(),lr=self.lr)\n",
    "        criterion = nn.MSELoss()\n",
    "        l = len(dataloader)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            p_bar = tqdm(dataloader)\n",
    "            for i,(images,_) in enumerate(p_bar):\n",
    "                images = images.to(self.device)\n",
    "                t = self.sample_timestemps(images.shape[0]).to(self.device)\n",
    "                x_t,noise = self.noise_images(images,t)\n",
    "                predicted_noise = unet_model(x_t,t)\n",
    "                loss = criterion(noise,predicted_noise)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                p_bar.set_postfix(MSE=loss.item())\n",
    "            sampled_images = self.generate(unet_model,n=images.shape[0])\n",
    "            self.save_images(sampled_images,os.path.join(\"results\",f\"{epoch}.jpg\"))\n",
    "            torch.save(unet_model.state_dict(),f\"ckpt.pt\")\n",
    "\n",
    "\n",
    "    def generate(self,unet_model,n):\n",
    "        unet_model.eval()\n",
    "        with torch.no_grad():\n",
    "            x = torch.randn((n,1,self.image_size,self.image_size)).to(self.device)\n",
    "            for i in tqdm(reversed(range(1,self.num_steps))):\n",
    "                t = (torch.ones(n)*i).long().to(self.device)\n",
    "                predicted_noise = unet_model(x,t)\n",
    "                alpha = self.alpha[t][:,None,None,None]\n",
    "                alpha_hat = self.alpha_hat[t][:,None,None,None]\n",
    "                beta = self.beta[t][:,None,None,None]\n",
    "                if i>1:\n",
    "                    noise = torch.randn_like(x)\n",
    "                else:\n",
    "                    noise = torch.zeros_like(x)\n",
    "                x = 1/ torch.sqrt(alpha) * (x-((1-alpha)/(torch.sqrt(1-alpha_hat)))* predicted_noise) + torch.sqrt(beta)*noise \n",
    "            unet_model.train()\n",
    "            unet_model.train()\n",
    "            x = (x.clamp(-1,1)+1)/2\n",
    "            x = (x * 255).type(torch.uint8)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69b7501c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DDPM = DiffusionModel()\n",
    "# DDPM.train(\n",
    "#     Unet(),\n",
    "#     MNIST_LOADER,\n",
    "#     EPOCHS,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23d03e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = DDPM.alpha \n",
    "alpha_hats = DDPM.alpha_hat\n",
    "betas = DDPM.beta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb44a9e",
   "metadata": {},
   "source": [
    "# Gif for posting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbbfcee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "99it [00:26,  3.77it/s]\n",
      "/var/folders/lv/z7k5n7b97ksfmphdq4bc_2mc0000gp/T/ipykernel_58303/1018528572.py:29: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  frames.append(imageio.imread(filename))\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('final'):\n",
    "    shutil.rmtree('final')\n",
    "os.mkdir('final')\n",
    "\n",
    "num_images = 32\n",
    "model = Unet()\n",
    "model.load_state_dict(torch.load(\"ckpt.pt\",map_location=DEVICE))\n",
    "x = torch.randn((num_images,1,28,28)).to(DEVICE)\n",
    "for i in tqdm(reversed(range(1,NUM_STEPS))):\n",
    "            t = (torch.ones(1)*i).long().to(DEVICE)\n",
    "            predicted_noise = model(x,t)\n",
    "            alpha = alphas[t][:,None,None,None]\n",
    "            alpha_hat = alpha_hats[t][:,None,None,None]\n",
    "            beta = betas[t][:,None,None,None]\n",
    "            if i>1:\n",
    "                    noise = torch.randn_like(x)\n",
    "            else:\n",
    "                    noise = torch.zeros_like(x)\n",
    "            x = 1/ torch.sqrt(alpha) * (x-((1-alpha)/(torch.sqrt(1-alpha_hat)))* predicted_noise) + torch.sqrt(beta)*noise\n",
    "            img = (x.clamp(-1,1)+1)/2\n",
    "            img = (img * 255).type(torch.uint8)\n",
    "            DDPM.save_images(img,os.path.join(\"final\",f\"{i}.jpg\"))\n",
    "\n",
    "\n",
    "frames = []\n",
    "for i in range(NUM_STEPS-1, 0, -1): \n",
    "    filename = f\"final/{i}.jpg\"\n",
    "    if os.path.exists(filename):\n",
    "        frames.append(imageio.imread(filename))\n",
    "imageio.mimsave(\"final.gif\", frames, duration=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da23d45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
